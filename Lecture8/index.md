## Lecture 8 - Attention/Transformers in NLP
Teacher: Romain Bielawski (ANITI)

### Lecture video
View the recorded lecture [here](https://drive.google.com/file/d/1YeF7TwHO5TbxVDuNYRUI-2P4Vy-sZj1s/view?usp=sharing)  (this will only be available for approximately 6 weeks after the course)

### Contents

* Attention in LSTM
* Self attention and transformer
* Embedding : Bert
* Generation : GPT2
* Application : Grammatical correctness classification, text generation, Movie plot generation


### Prerequisites:
Knowledge about neural networks principles; knowledge of several NN layer types; gradient descent & backpropagation; basics of NLP; RNN principles

## Slides

Download the slides [here](https://docs.google.com/presentation/d/1U8CcFwYOmnplEA3qqjGsM7wi0nZ2nGwBtegW2z2Lgd4/edit?usp=sharing)

### Notebook
Access the collab notebook [here](https://colab.research.google.com/drive/14EV6vfTECPxq_xG9xajAxvd6tBmiGLFf?usp=sharing)

### Further reading:

---
#### [(Back to Main Page)](../index.md)
