## Lecture 7 - Recurrent Neural Networks for text processing
Teacher: Romain Bielawski (ANITI)

### Lecture video
View the recorded lecture [here](https://drive.google.com/file/d/1Gs8LSEkDLlXxj_rRPwOt-dN20r19n0wo/view?usp=sharing)  (this will only be available for approximately 6 weeks after the course)

### Contents

* Sequential data and variable size inputs
* Recurrent neural network principles, hidden states
* Backprop through time
* RNN, LSTM, GRU
* Sentence embeddings, text generation, seq2seq, machine translation
* Application : LSTM for text generation

## Slides

Download the slides [here](https://docs.google.com/presentation/d/1KS1_n3JO_4yZ1xCn8tDjkGPiSr2L1RIRN4SVFcXrhgk/edit?usp=sharing)

### Notebook
Access the collab notebook [here](https://colab.research.google.com/drive/1MWqyE46KXhKfL-hpQBMhAHYSlsKejtal?usp=sharing)


### Prerequisites:
Knowledge about neural networks principles; knowledge of several NN layer types; gradient descent & backpropagation; basics of NLP


### Further reading:

---
#### [(Back to Main Page)](../index.md)
