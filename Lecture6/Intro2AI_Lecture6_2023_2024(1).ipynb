{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT3dnp1IkNR0"
      },
      "source": [
        "# Intro2AI: Lecture 6 - Introduction to Natural Language Processing\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1jiB_rcx8777OqnMHcll2jYJQZF-sIaPm)\n",
        "\n",
        "In this practical session, we will see how to:\n",
        "- Pre-process data using Spacy\n",
        "- Build a sentiment analysis system, using Scikit-Learn\n",
        "- Generate word embeddings, using Gensim\n",
        "\n",
        "Our corpus for sentiment classification will be the Pop-corn dataset ( https://www.kaggle.com/ymanojkumar023/kumarmanoj-bag-of-words-meets-bags-of-popcorn/code).\n",
        "\n",
        "\n",
        "We make available a reduced and cleaned version here (code to download the data below):\n",
        "- Training set: https://drive.google.com/file/d/1VcPE4bo8ygubyLmxwA-jycckUtVcC6l6/view?usp=sharing\n",
        "- Test set: https://drive.google.com/file/d/1GQf17s5Tf7rXobjhDON9gek2gOHyJ4-K/view?usp=sharing\n",
        "- Full data for training word embeddings: https://drive.google.com/file/d/1TokJd_dnYksfHjCqMpKEqrLHDhkwSFyV/view?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Organization of the session: please read\n",
        "\n",
        "The practical session is organized in several parts, with some code to run and a detailed explanation.\n",
        "You simply have to read and run the code, the goal is to make you understand that NLP systems are not perfect, but also how easy it is to build a simple classification system (with rather good performance).\n",
        "Each time try to **understand the code and its output**: ask me question whenever something is unclear."
      ],
      "metadata": {
        "id": "6WSV2qAVcX2o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLwHu5tY8LGx"
      },
      "source": [
        "# install wget, a module used to download data from the web\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9zNG9vz3gN8"
      },
      "source": [
        "# Downloading data\n",
        "# wiki_ai.txt\n",
        "#wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1dayJql47Thz8dmOF1txyQj77FoktWtGo' -O wiki_ai.txt\n",
        "\n",
        "import wget\n",
        "# wiki_ai.txt (for Part 1)\n",
        "url = \"https://docs.google.com/uc?export=download&id=1dayJql47Thz8dmOF1txyQj77FoktWtGo\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Training data (for Part 2)\n",
        "url = \"https://docs.google.com/uc?export=download&id=1VcPE4bo8ygubyLmxwA-jycckUtVcC6l6\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Test data (for Part 2)\n",
        "url = \"https://docs.google.com/uc?export=download&id=1GQf17s5Tf7rXobjhDON9gek2gOHyJ4-K\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Full dataset (for Part 3)\n",
        "url = \"https://docs.google.com/uc?export=download&id=1BzE2l8R51ONSp3ADvXXukuYyuAgobnMW\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Original dataset (fyi, not used in this Practical Session)\n",
        "#url = \"https://docs.google.com/uc?export=download&id=1HFGLcWDn_vcmze-L_0jzB40ybmnD2_c1\"\n",
        "#filename = wget.download(url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEoTsGiehLLa"
      },
      "source": [
        "# Part 1: Using Spacy for data pre-processing\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1L9JLLQHPZoMRwzYfmKcyM9VME_SHeZrr)\n",
        "\n",
        "Within a computer, text is encoded as a string of characters.\n",
        "In order to analyze textual data within NLP applications, we first need to properly preprocess it.\n",
        "An NLP preprocessing pipeline generally consists of the following steps :\n",
        "* sentence segmentation\n",
        "* tokenisation\n",
        "* normalization: lower-casing, lemmatization, removing stop-words\n",
        "* pos-tagging\n",
        "* named entity recognition\n",
        "* parsing\n",
        "\n",
        "The first two steps are in general necessary, while the others are optional.\n",
        "\n",
        "For these exercises, we will use the module **spacy** (already installed on google colab): https://spacy.io/api/doc\n",
        "\n",
        "Spacy is a python module that implements an NLP pipeline, in order to carry out tasks such as segmentation, tokenization, lemmatization and pos-tagging.\n",
        "We will use it in order to preprocess a document in English.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The text comes from Wikipedia: https://www.wikiwand.com/en/Artificial_intelligence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubFamr89vq_Z"
      },
      "source": [
        "# The code below opens a file, read, save and print its content\n",
        "with open( 'wiki_ai.txt') as infile:\n",
        "  text = infile.read()\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUnGCI8Ad8r3"
      },
      "source": [
        "## 1.1 Tokenisation\n",
        "\n",
        "Spacy can be used to directly tokenize any text.\n",
        "To make it work, you need to load a model specific to the target language, here 'en_core_web_sm' for English. There are also some domain specific models, and models for other languages: https://spacy.io/models/en\n",
        "\n",
        "\n",
        "This model corresponds to a processing 'pipeline', including, depending of the model: tokenisation, lemmatization, POS tagging, Named Entity Recognition and parsing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import and load\n",
        "\n",
        "The code below is used to:\n",
        "- import the spacy module into Python\n",
        "- load all the necessary models for English (other models can be found here: https://spacy.io/usage/models)"
      ],
      "metadata": {
        "id": "H4yOFk7TUAw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "R4BD2UfhTVA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read and process a text\n",
        "The code below  is used to:\n",
        "- open the file 'wiki_ai.txt' for reading\n",
        "- process it using spacy’s nlp pipeline"
      ],
      "metadata": {
        "id": "W4dwbKXwUNGp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q1L4ApskpmF"
      },
      "source": [
        "# Read in string of characters\n",
        "with open('wiki_ai.txt') as inFile:\n",
        "    text = inFile.read()\n",
        "\n",
        "# Preprocess using spacy's pipeline\n",
        "doc = nlp(text)\n",
        "\n",
        "print('Preprocessing done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3FdmUgnw5Ox"
      },
      "source": [
        "### Inspect tokens\n",
        "\n",
        "Our preprocessed document is now present as a list of tokens in our doc variable, and we can access its different annotations by looping through it.\n",
        "\n",
        "The code below  is used to:\n",
        "- print each individual token, together with its lemmatized form and part of speech tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxMTGR8sw_9i"
      },
      "source": [
        "# Inspect tokens, lemmas, and pos tags\n",
        "for token in doc:\n",
        "  print( token.text, token.lemma_, token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HcJhX3DfCTm"
      },
      "source": [
        "### Vizualizing using Pandas\n",
        "\n",
        "You can use Pandas, another Python library, to better visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VW_9Glf1Gsz"
      },
      "source": [
        "# Using pandas for a better visualization\n",
        "import pandas as pd\n",
        "\n",
        "spacy_pos_tagged = [(w, w.tag_, w.pos_) for w in doc]\n",
        "pd.DataFrame(spacy_pos_tagged,\n",
        "             columns=['Word', 'POS tag', 'Tag type'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8EJ7iH2mUo-"
      },
      "source": [
        "### Look at the results\n",
        "\n",
        "Do you see some errors?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjqXNiCOnvlz"
      },
      "source": [
        "### POS tags explanation\n",
        "\n",
        "* You can use the method 'explain' to have information about some annotation, for example the POS tags, see the code below.\n",
        "* Here we used a very small set of POS (vs e.g. 36 in the PTB: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXStTWgunYqT"
      },
      "source": [
        "# Inspect POS tags\n",
        "all_tags = set()\n",
        "for token in doc:\n",
        "  all_tags.add(token.pos_)\n",
        "for tag in all_tags:\n",
        "  print( tag, spacy.explain(tag)) # explain each label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dC9lMGoZIc"
      },
      "source": [
        "## 1.2 Segmenting into sentences\n",
        "\n",
        "Apart from token segmentation, Spacy has also automatically segmented our document into sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing sentences\n",
        "The code below can be used to print out the different sentences of the document. Do you see any error?"
      ],
      "metadata": {
        "id": "JIQWpNCTeT89"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQmuXqxuoyVC"
      },
      "source": [
        "# Print the sentences\n",
        "for i, sent in enumerate( doc.sents ):\n",
        "  print( i, sent.text.strip() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-1IyOOTpkML"
      },
      "source": [
        "## 1.3 Named entity recognition\n",
        "\n",
        "As part of the preprocessing pipeline, Spacy has equally carried out named entity recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing Named Entities\n",
        "\n",
        "The code below will:\n",
        "* print out each named entity, together with the label assigned to it"
      ],
      "metadata": {
        "id": "d4RqYoEpeYpo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOu1lpzop19t"
      },
      "source": [
        "entity_labels = set()\n",
        "for entity in doc.ents:\n",
        "  label = entity.label_\n",
        "  print( entity.text, '\\t', label )\n",
        "  entity_labels.add( label )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do the labels stand for? We can use the method 'explain' again:"
      ],
      "metadata": {
        "id": "QcmTZ_H5dsG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for l in entity_labels:\n",
        "  print( l, spacy.explain(l))"
      ],
      "metadata": {
        "id": "8d3Wz8-7dwwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPGZqq3sfyNm"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "A module called 'displacy' can be used to visualize the Named Entities directly in the text. It's easier to read.\n",
        "\n",
        "Can you see some errors?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U04k54xT0ORj"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Visually\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q6a-haVzKTS"
      },
      "source": [
        "## 1.4 Syntactic parsing\n",
        "\n",
        "Syntactic parsers produce an analysis of the sentences, where the words are connected to each other through syntactic relations.\n",
        "We can easily parse sentences with Spacy, in order to produce a dependency graph over the sentences.\n",
        "The dependency relations can be used as features for other systems, to know who did what, or to know which word is modified by an adjective.\n",
        "\n",
        "More info: https://spacy.io/usage/linguistic-features#dependency-parse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing a parse tree\n",
        "\n",
        "The code below will:\n",
        "- import Spacy and load the model\n",
        "- process the sentence using the spacy pipeline\n",
        "- vizualise the parse tree with displacy"
      ],
      "metadata": {
        "id": "RingVByEYgZj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZE9gtNAzOT4"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "#nlp = spacy.load('en')\n",
        "example_sentence = \"You can make dependency trees.\"\n",
        "example_doc = nlp(example_sentence)\n",
        "\n",
        "# Visualization\n",
        "displacy.render(example_doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will:\n",
        "- iterate over the sentence in the wikipedia document\n",
        "- print the parse tree of the first sentence (index 0)"
      ],
      "metadata": {
        "id": "kkRTvVpBZVNa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NZMWtkYzT0a"
      },
      "source": [
        "# Print the first sentence of our document\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(sentences[0])\n",
        "doc = nlp(sentences[0])\n",
        "\n",
        "# Visualization\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFXSO1GDgQJY"
      },
      "source": [
        "### Navigating the parse tree\n",
        "\n",
        "Each element of the tree is associated to attributes: you can use them to inspect the different elements of the trees.\n",
        "\n",
        "The code below will print a tabular version of the tree where each token id associated to its head, with the relation ('amod') between them. The eventual children of the current token are also printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB2ceahj2ivz"
      },
      "source": [
        "# Navigating the parse tree\n",
        "spacy_dep_rel = [(w.text, w.dep_, w.head.text, w.head.pos_, [child.text for child in w.children]) for w in doc]\n",
        "pd.DataFrame(spacy_dep_rel,\n",
        "             columns=['Word', 'Dep', 'Head text', 'Head pos', 'children'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-T2diU3q7B9"
      },
      "source": [
        "# Part 2: Sentiment analysis, \"Bag of Words Meets Bags of Popcorn\"\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13nwT3niIwy8jJKEyTF0dRaHeEi1q8Zlv)\n",
        "\n",
        "In this part, we will make experiments on sentiment analysis on movie reviews.\n",
        "The reviews are either positive (label 1) or negative (label 0).\n",
        "\n",
        "The data come from: https://www.kaggle.com/ymanojkumar023/kumarmanoj-bag-of-words-meets-bags-of-popcorn/code\n",
        "\n",
        "In this part, we will:\n",
        "- vectorize the data using a bag-of-word representation\n",
        "- train and evaluate a classifier for sentiment analysis.\n",
        "\n",
        "To this aim, we will use the **scikit-learn** library.\n",
        "It is already installed within google colab (https://scikit-learn.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNuTf1mTLYfq"
      },
      "source": [
        "## 2.1 Retrieving the data\n",
        "\n",
        "The data have already been tokenized and normalized (i.e. lowercased).\n",
        "Data are balanced: there is an equal number of positive and negative examples in both the training an test set.\n",
        "We have 5000 training instances, and 500 test instances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data\n",
        "The code below will:\n",
        "- read the data\n",
        "- print the first instances: do the labels seem correct?"
      ],
      "metadata": {
        "id": "QH2phzC7ePrZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmUjkWJfaNGM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Read data using panda\n",
        "import pandas as pd\n",
        "\n",
        "def read_data( infile ):\n",
        "  data = pd.read_csv(infile, header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "  print(\"Number of examples:\", data.shape[0],\"\\n\")\n",
        "\n",
        "  reviews = data[\"review\"]\n",
        "  labels = data[\"sentiment\"]\n",
        "  return data, reviews, labels\n",
        "\n",
        "print( \"\\n-- Reading training data \")\n",
        "train, train_reviews, train_labels =read_data( \"popcorn_clean_train_5000.tsv\" )\n",
        "\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJxuy14Xtrhr"
      },
      "source": [
        "## 2.2 Feature extraction\n",
        "\n",
        "Now, we are going to transform our textual data into vectors.\n",
        "We'll start with simple bag-of-words features.\n",
        "\n",
        "The class CountVectorizer implements this transformation:\n",
        "- It converts a collection of text documents to a matrix of token counts (= raw frequency): each document become a numerical vector.\n",
        "  \n",
        "\n",
        "### How to vectorize data\n",
        "\n",
        "To transform your data, you need to:\n",
        "- (1) build a CountVectorizer object with the desired options\n",
        "```\n",
        "vectorizer = CountVectorizer( analyzer = 'word', max_features=1000 )\n",
        "```\n",
        "- (2) learn the transformation on your input data\n",
        "```\n",
        "vectorizer.fit( train_reviews )\n",
        "```\n",
        "- (3) transform your data into the desired output\n",
        "```\n",
        "train_features = vectorizer.transform( train_reviews )\n",
        "```\n",
        "\n",
        "Note that the method \"fit_transform\" automatically learns AND applies the transformation to the input data (steps (2) and (3)).\n",
        "```\n",
        "train_features = vectorizer.fit_transform( train_reviews )\n",
        "```\n",
        "\n",
        "### Filtering\n",
        "\n",
        "Without filtering, this produces vectors of **39328 dimensions!**\n",
        "Here we arbitrarily reduce to 1000 (but other values should be tested).\n",
        "- max_features=1000: build a vocabulary that only consider the top 1000 features ordered by term frequency across the corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize data\n",
        "\n",
        "The code below will:\n",
        "- import the required module\n",
        "- create an instance of CountVectorizer, used to build vectors from text\n",
        "- vectorize the training set (i.e. the text 'train_reviews' is transformed into numerical vectors in 'train_features')"
      ],
      "metadata": {
        "id": "VLOThU_ucC7b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp7Y0kEt43fV"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer( analyzer = 'word', max_features=1000 )\n",
        "train_features = vectorizer.fit_transform( train_reviews )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STgyakb3TQO4"
      },
      "source": [
        "### Look at the vectorization\n",
        "\n",
        "The code below will print several information, check that you understand each part:\n",
        "\n",
        "- a- Print the shape of the matrix, that is the set of vectors representing the reviews (nb of instances x nb of features)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtYx-O8-TOFl"
      },
      "source": [
        "# a- array of shape (n_samples, n_features)\n",
        "print( \"Shape of the data, ie nb of examples x number of features:\", train_features.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- b- Print the vocabulary, i.e. the unique words used as features"
      ],
      "metadata": {
        "id": "Rg6x-XuIcmro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# b- print the vocabulary (= unique words, here 1000)\n",
        "print( \"\\nVocabulary:\", list( vectorizer.vocabulary_.keys() ) )\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print( \"Size of the vocabulary:\", len(vocab))\n",
        "print(  \"Sorted vocabulary:\", vocab[:10] )"
      ],
      "metadata": {
        "id": "f8bfK4N1cm8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- c- Print the vector representing the first review"
      ],
      "metadata": {
        "id": "i91oVY1JdE1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# c- print the vector representing the first review (500 dimensions)\n",
        "# use toarray() to densify the matrix --> many 0s = sparsity\n",
        "print( \"\\nVector representing the first review\", train_features[0].toarray())"
      ],
      "metadata": {
        "id": "AJo4NY6kdE1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- d- Print the word corresponding to the first non-zero dimension, here dimension 6 (index = 5). Check that it appears once in the first review."
      ],
      "metadata": {
        "id": "-idU7P6adNVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d- sixth dimension, value =5\n",
        "# what is the corresponding word?\n",
        "# - invert the dictionnary\n",
        "index_to_token = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
        "print( \"\\nWord corresponding to the 5th dimension:\", index_to_token[5])\n",
        "print( \"First review\", train_reviews[0]) # 2nd sentence: \"some human drama about what could\""
      ],
      "metadata": {
        "id": "UZ5SbE01dOAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pDzO4QLO5br"
      },
      "source": [
        "## 2.3 Preparing test data\n",
        "\n",
        "We also need to pre-process and vectorize the test set.\n",
        "\n",
        "The difference is:\n",
        "- the vectorization is 'learned' on the training data only, we use the 'transform' method of the vectorizer (without the 'fit' part): words that do not appear in our training set are considered 'unknown'."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test data\n",
        "\n",
        "The code below will prepare the test data (note that we should use development data instead, see last part for more details)."
      ],
      "metadata": {
        "id": "i9s8YrqDgRFN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPg0TnCNPPgw"
      },
      "source": [
        "print( \"-- Reading test data \")\n",
        "test, test_reviews, test_labels = read_data( \"popcorn_clean_test_500.tsv\" )\n",
        "\n",
        "test_features = vectorizer.transform( test_reviews )\n",
        "print( \"Vectorized, shape:\", test_features.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK9SoDbUzo_c"
      },
      "source": [
        "## 2.4 Classification without neurons: Scikit-Learn\n",
        "\n",
        "Now we can train a model and use to make predictions on our test set.\n",
        "- Choose an algorithm, e.g. LogisticRegression (aka MaxEnt)\n",
        "- Train on the training set, meaning that we fit the model to the training data\n",
        "- Make predictions on the development set\n",
        "- Report performance by comparing the gold labels from the evaluation set (i.e. test_labels) to the predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gClrPWfVjEp_"
      },
      "source": [
        "### Step 1- Training\n",
        "\n",
        "The code below will:\n",
        "- import the required module\n",
        "- initialize a classifier based on logistic regression\n",
        "- train/fit the classifier using the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiaxzD8u426g"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features, train_labels )\n",
        "print( 'Training done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4V_WSfmVGCd"
      },
      "source": [
        "### Step 2- Making predictions\n",
        "\n",
        "The code below uses the model learned (the classifier) to make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plc6V1FCVKhz"
      },
      "source": [
        "preds = classifier.predict( test_features )\n",
        "print( \"Prediction done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr_bWy1hVJHo"
      },
      "source": [
        "### Step 3- Computing scores\n",
        "\n",
        "Scoring is done by comparing the gold labels (i.e. the ones annotated by an human) to the predicted labels assigned by the model.\n",
        "\n",
        "Scikit-learn provides a method called \"classification_report\" that gives an overview of the performance using different metrics, as done in the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djLyDS2IVUCS"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG2WhhvRok7F"
      },
      "source": [
        "## 2.5 Improving the results: modifying data representation\n",
        "\n",
        "Many parts of a model can be modified to try to improve the performance:\n",
        "- the data representation\n",
        "- the values of the hyper-parameters (in Part 4)\n",
        "- the choice of the algorithm (in Part 4)\n",
        "\n",
        "Data representation corresponds to the choice of features.\n",
        "Here, we choosed a simple bag-of-word representation (BOW) with raw frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7EZMS9pxBq"
      },
      "source": [
        "### TF-IDF normalization\n",
        "\n",
        "As said during the course, BOW comes with many flavors, and a good option in general is to use TF-IDF normalization instead of raw features.\n",
        "\n",
        "With scikit, you can either directly vectorize using TF-IDF (with the class 'TfidfVectorizer') or transform a count-based representation (with the class 'TfidfTransformer').\n",
        "Here, we use this second option.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The code below will produce training and test sets with TFIDF representations.\n",
        "The next cell witll train and evaluate the classifier.\n",
        "\n",
        "- Look how it changed the values in the vector representing the first review.\n",
        "- Does it change the performance?"
      ],
      "metadata": {
        "id": "TIFgSVWpvN56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer()\n",
        "train_features_tfidf = transformer.fit_transform(train_features)\n",
        "test_features_tfidf = transformer.transform(test_features)\n",
        "\n",
        "# Print the vector representing the first review (500 dimensions)\n",
        "# use toarray() to densify the matrix --> many 0s = sparsity\n",
        "print( \"\\nVector representing the first review\", train_features_tfidf[0].toarray())"
      ],
      "metadata": {
        "id": "B3MCh3Jdu9f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUXVnDJcp8sI"
      },
      "source": [
        "# Training\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features_tfidf, train_labels )\n",
        "# Predictions\n",
        "preds = classifier.predict( test_features_tfidf )\n",
        "# Scores\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtSeIuQp9K_"
      },
      "source": [
        "### Tri-grams features\n",
        "\n",
        " As said during the course, BOW doesn't take into account the context of each word, which can be crucial for the task.\n",
        "\n",
        " Let's try with tri-grams, or here, more specifically a concatenation of:\n",
        " - unigrams: single tokens, same as BOW\n",
        " - bigrams: two words\n",
        " - trigrams: three words\n",
        " This is done with the option 'ngram_range'.\n",
        "\n",
        " Note that here we directly take the TF-IDF vectorizer.\n",
        "\n",
        " Without filtering, this produces vectors of **1366006 dimensions**! Here, we choose to keep 5000 features, more than previously to take into account the new features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below combines:\n",
        "- Building a representation of the data built on tri-grams\n",
        "- Train a classifier\n",
        "- Make predictions\n",
        "- Print scores\n",
        "\n",
        "Do you see any improvement?"
      ],
      "metadata": {
        "id": "g5noO0x_ylMP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93BYAfjdOcW"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer( analyzer = 'word', max_features = 5000, ngram_range=(1,3) )\n",
        "train_features_tfidf_ngram = vectorizer.fit_transform( train_reviews )\n",
        "test_features_tfidf_ngram = vectorizer.transform( test_reviews )\n",
        "\n",
        "print( train_features_tfidf_ngram.shape, test_features_tfidf_ngram.shape)\n",
        "\n",
        "# Training\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features_tfidf_ngram, train_labels )\n",
        "# Predictions\n",
        "preds = classifier.predict( test_features_tfidf_ngram )\n",
        "# Scores\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary size\n",
        "\n",
        "The code below will print the vocabulary for this representation of the data: can you see what has changed?"
      ],
      "metadata": {
        "id": "0-kMGPBjkfqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the vocabulary (= unique words, here 5000)\n",
        "print( \"\\nVocabulary:\", list( vectorizer.vocabulary_.keys() ) )\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print( \"Size of the vocabulary:\", len(vocab))\n",
        "print(  \"Sorted vocabulary:\", vocab[:100] )"
      ],
      "metadata": {
        "id": "cpTkWEEdkHrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeVgZiOBd7pB"
      },
      "source": [
        "## 2.6 Inspecting the model\n",
        "\n",
        "The linear classifiers work by learning weights over the features.\n",
        "Looking at these weights can give some insights on your model.\n",
        "\n",
        "With LogisticRegression in the binary setting, we have:\n",
        "- the most positive weights are the best indicators of the positive class (here positive reviews)\n",
        "- the most negative weights are the best indicators of the negative class (here negative reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing positive and negative features\n",
        "\n",
        "The code below will print the 50 most positive and negative features: do the results make sense?"
      ],
      "metadata": {
        "id": "AV79intWhV4Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfSxjmu6d9tJ"
      },
      "source": [
        "# Here we look at the best model obtained with grid search, ngrams features and tf idf normalization\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "allCoefficients = [(classifier.coef_[0,i], vocab[i]) for i in range(len(vocab))]\n",
        "allCoefficients.sort()\n",
        "allCoefficients.reverse()\n",
        "\n",
        "print(\"Top features for positive class:\")\n",
        "print( '\\n'.join( [ f+':\\t'+str((round(w,3))) for (w,f) in allCoefficients[:50]] ) )\n",
        "\n",
        "print(\"\\nTop features for negative class:\")\n",
        "print( '\\n'.join( [ f+':'+str((round(w,3))) for (w,f) in allCoefficients[-50:]] ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error analysis\n",
        "\n",
        "The code below will print the errors of our system. Look at some examples, what do you think?"
      ],
      "metadata": {
        "id": "uBglZ-tim_NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the errors from the systems, ie reviews that were wrongly classified\n",
        "pos_as_neg = []\n",
        "neg_as_pos = []\n",
        "for i, r in enumerate( test_reviews):\n",
        "  if test_labels[i] != preds[i]:\n",
        "    if test_labels[i] == 1:\n",
        "      pos_as_neg.append( [test_labels[i], preds[i], r] )\n",
        "    else:\n",
        "      neg_as_pos.append( [test_labels[i], preds[i], r] )\n",
        "print( \"------ Positive reviews that have been wrongly predicted as negative: \"+str(len(pos_as_neg))+\"\\n\")\n",
        "print( '\\n'.join( [r for g,p,r in pos_as_neg] ))\n",
        "\n",
        "print( \"\\n\\n------ Negative reviews that have been wrongly predicted as positive: \"+str(len(neg_as_pos))+\"\\n\")\n",
        "print( '\\n'.join( [r for g,p,r in neg_as_pos] ))"
      ],
      "metadata": {
        "id": "DWoGU0_Nk-Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It2say-ofOOn"
      },
      "source": [
        "# Part 3: generating word embeddings\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1eLkKWp8yOP6AJsK2h6Btbr3L95TvDsyD)\n",
        "\n",
        "\n",
        "\n",
        "As introduced during the course, we can use neural networks to generate vectors representing words.\n",
        "These vectors, learned on massive amount of data, allow to compute similarity measures between words.\n",
        "\n",
        "As an introductive exercise, we will generate word embeddings from the sentiment review dataset and take a look at the generated vectors.\n",
        "\n",
        "Remind that this corpus is \"small\", compared to what is generally used for generating embeddings, here around 40k words against millions of words in general!\n",
        "The resulting vectors will thus not be of extremely good quality (but the model will run very fast :)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cANCUNNJf_oM"
      },
      "source": [
        "## 3.1 Generating word embeddings\n",
        "\n",
        "We  will  use gensim in  order  to  induce  word  embeddings  from  text.\n",
        "gensim is  a  vector  space modeling and topic modeling toolkit for python, and contains an efficient implementation of the word2vec algorithms.\n",
        "\n",
        "word2vec consists of two different algorithms: skipgram (sg) and continuous-bag-of-words (cbow).\n",
        "The underlying prediction task of the former is to estimate the context words from the target word ; the prediction task of the latter is to estimate the target word from the sum of the context words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSgCIkdRfRsZ"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "import gzip\n",
        "import logging\n",
        "\n",
        "import time\n",
        "\n",
        "# set up logging for gensim\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)\n",
        "\n",
        "# we define a PlainTextCorpus class; this will provide us with an\n",
        "# iterator over the corpus (so that we don't have to load the corpus\n",
        "# into memory)\n",
        "class PlainTextCorpus(object):\n",
        "    def __init__(self, fileName):\n",
        "        self.fileName = fileName\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in gzip.open(self.fileName, 'rt', encoding='utf-8'):\n",
        "            yield  line.split()\n",
        "\n",
        "# instantiate the corpus class using corpus location\n",
        "#sentences = PlainTextCorpus('raw_reviews.txt.gz')\n",
        "sentences = PlainTextCorpus('raw_reviews_cleaned2.txt.gz')\n",
        "\n",
        "# we only take into account words with a frequency of at least 50, and\n",
        "# we iterate over the corpus only once\n",
        "model = Word2Vec(sentences, min_count=50, epochs=1)\n",
        "\n",
        "# finally, save the constructed model to disk\n",
        "model.save('model_word2vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CNItSggh2P"
      },
      "source": [
        "## 3.2 Compute word similarity\n",
        "\n",
        "You can now compute the most similar words (which is measured by cosine similarity between the word vectors) by issuing the following command:\n",
        "\n",
        "model.wv.most_similar(myword)\n",
        "\n",
        "Don't hesitate to test with other words, such as \"movie\", \"good\" etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGUQeqZxgrne"
      },
      "source": [
        "model.wv.most_similar('actor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5cOuEpIn7tY"
      },
      "source": [
        "model.wv.most_similar('romance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2MAHA5ug6mk"
      },
      "source": [
        "Word  embeddings  allow  us  to  do  analogical  reasoning  using  vector  addition and subtraction.\n",
        "gensim offers the possibility to do so.\n",
        "\n",
        "Try to perform analogical reasoning,  e.g.  actor - man  +  woman  = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6IMcawVhP0p"
      },
      "source": [
        "model.wv.most_similar(positive=[\"actor\", \"woman\"], negative=[\"man\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-b2PschV81"
      },
      "source": [
        "## 3.3 Modify the model\n",
        "\n",
        "As a default, the word2vec module creates word embeddings with the following setting:\n",
        "- algorithm: CBOW\n",
        "- window: 5\n",
        "- embeddings size: 100\n",
        "\n",
        "Try other options, including:\n",
        "- algorithm: skipgram\n",
        "- window: try varied sizes, from very small to large one\n",
        "- embeddings size: try varied sizes, from very small to large one\n",
        "\n",
        "Each time, evaluate the impact on the similarity computation.\n",
        "What configuration works best?\n",
        "\n",
        "See doc: https://radimrehurek.com/gensim_3.8.3/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r_3PC-oiu-d"
      },
      "source": [
        "# a- MODIFYING THE WINDOW SIZE (here 1)\n",
        "\n",
        "model_w1 = Word2Vec(sentences, min_count=50, epochs=1, window=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRXKwejKjuQ-"
      },
      "source": [
        "# a- MODIFYING THE WINDOW SIZE (here 20)\n",
        "\n",
        "model_w20 = Word2Vec(sentences, min_count=50, epochs=1, window=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PurMhdRSi7OD"
      },
      "source": [
        "# b- MODIFYING THE EMBEDDINGS SIZE (here 10)\n",
        "\n",
        "model_s10 = Word2Vec(sentences, min_count=50, epochs=1, vector_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKu6Ot6tjRFe"
      },
      "source": [
        "# b- MODIFYING THE EMBEDDINGS SIZE (here 300)\n",
        "\n",
        "model_s300 = Word2Vec(sentences, min_count=50, epochs=1, vector_size=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGAe2UXaiB8z"
      },
      "source": [
        "# c- WITH SKIPGRAM\n",
        "\n",
        "# we only take into account words with a frequency of at least 50, and\n",
        "# we iterate over the corpus only once\n",
        "model_sg = Word2Vec(sentences, min_count=50, epochs=1, sg=1)\n",
        "\n",
        "# finally, save the constructed model to disk\n",
        "model.save('model_word2vec_sg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--H64gv7oLa4"
      },
      "source": [
        "# Results with CBOW\n",
        "model.wv.most_similar('romance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N8XDL0voToe"
      },
      "source": [
        "# Results with Skip-gram\n",
        "model_sg.wv.most_similar('romance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXjmWOd9nL5H"
      },
      "source": [
        "# PART 4: additional notes on classification\n",
        "\n",
        "## Finding the best model\n",
        "\n",
        "Usually, we will want to try out different parameters, in order to see what works best for our task. As such, we might experiment with:\n",
        "- Different features\n",
        "- Different classification algorithms\n",
        "- Different model parameters\n",
        "\n",
        "However, we have to be careful: we cannot use our test set over and over again, as we’ll be optimizing our parameters for that particular test set, and run the risk of overfitting, which means we are not able to properly generalize to data we haven’t trained on.\n",
        "We want to build a model that is robust, meaning that it will get good performance on unseen data.\n",
        "That's why we only use the test set at the end, with the best model.\n",
        "\n",
        "For this reason, we need to make use of a validation our development set.\n",
        "However, our training set is already quite small; creating a separate validation set would give us even less training data.\n",
        "\n",
        "Fortunately, there is another option: we can use k-fold cross validation.\n",
        "The idea is the following:\n",
        "- Break up data into k (e.g. 10) parts (folds)\n",
        "- For each fold\n",
        "    - Current fold is used as temporary test set\n",
        "    - Use other 9 folds as training data\n",
        "    - Performance is computed on test fold\n",
        "- Average performance over 10 runs\n",
        "\n",
        "Scikit provides efficient ways of performing cross-fold validation.\n",
        "We will test below the grid search that allows to choose the best values for the hyper-parameters, using cross-validation over the trianing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EP2EnB1XOHF"
      },
      "source": [
        "### 4.1 Optimizing the hyper-parameters\n",
        "\n",
        "Each algorithm comes with some \"options\" called hyper-parameters.\n",
        "The chosen values can have an important effect on the results.  \n",
        "\n",
        "For example, Logistic Regression has:\n",
        "- 'C' a coefficient C used for regularization, with smaller values specifying stronger regularization.\n",
        "- 'max_iter' (default=100) Maximum number of iterations taken for the solvers to converge.\n",
        "\n",
        "Here we use the class 'GridSearchCV' that will perform an exhaustive search over specified parameter values for an estimator (i.e. a classifier).\n",
        "We specify the algorithm we want (here 'LogisticRegression') and the parameters values we want to test (see the dictionnary 'parameters').\n",
        "\n",
        "Then the 'fit' method over the GridSearchCV object allows to perform the search over the parameters, using a cross-fold validation (default: 5-fold CV).\n",
        "\n",
        "Then, you can print the best set of parameters and the best score (i.e. Mean cross-validated score of the best_estimator), and use a panda dataframe to visualize the results according to each set of parameters.\n",
        "\n",
        "\n",
        "See the doc: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUj_rkBmXR6t"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'C':[0.0001, 0.01, 0.1, 1, 100], 'max_iter':[1, 10] }\n",
        "lr = LogisticRegression()\n",
        "clf_lr = GridSearchCV(lr, parameters, verbose=1)\n",
        "clf_lr.fit( train_features_tfidf_ngram, train_labels )\n",
        "sorted(clf_lr.cv_results_.keys())\n",
        "\n",
        "print( \"Best parameters found:\", clf_lr.best_params_)\n",
        "print( \"Best score found:\", clf_lr.best_score_)\n",
        "\n",
        "pd.concat([pd.DataFrame(clf_lr.cv_results_[\"params\"]),pd.DataFrame(clf_lr.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v2gFaikpc3h"
      },
      "source": [
        "You can then directly use the GridSearchCV object (here called 'clf') to make predictions on your test set: it correspond to the best model found during the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il2DEVOpZuNT"
      },
      "source": [
        "preds = clf_lr.predict( test_features_tfidf_ngram )\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypnlIloLxjFO"
      },
      "source": [
        "## 4.2 Optional exercise: Try other algorithms\n",
        "\n",
        "Now, you can use the grid search to test another algorithm (e.g. Naive Bayes, SVM).\n",
        "You only need to perform the grid search, then report the results on the test set for the best algorithm only.\n",
        "\n",
        "Doc for Naive Bayes: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
        "\n",
        "Doc for SVM: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
        "\n",
        "Which one performs the best?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7h50aQdD8c9"
      },
      "source": [
        "# Testing Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQPGtmtfFSy2"
      },
      "source": [
        "# Testing SVM\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Notes: Other options for vectorization\n",
        "\n",
        "When converting our data into vectors using bag-of-words, many other options are implemented in scikit-learn, e.g.:\n",
        "- 'stop_words='english': will automatically remove stop-words from a list (but be careful: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words)\n",
        "- binary (default False): If True, all non zero counts are set to 1.\n",
        "- ngram_range (tuple (min_n, max_n), default=(1, 1)): The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
        "- analyzer='word': can be changed to 'char' if you want to use characters as features\n",
        "\n",
        "See the doc: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
      ],
      "metadata": {
        "id": "cTXQD7Qyfjaa"
      }
    }
  ]
}